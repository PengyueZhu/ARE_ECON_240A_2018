---
title: "6PM_assign4"
output:
  ioslides_presentation: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r , results = "hide", message=FALSE}
rm(list = ls())      # clean the workspace
library(tidyverse) #everything
library(lmtest) # coeftest
library(sandwich) # vcovHC
library(dplyr)
library(broom) # tidy the output
library(knitr) # For knitting document and include_graphics function
library(png)      # For grabbing the dimensions of png files
Simu_beta <- function(n){
  df <- data_frame(x = rnorm(n,0,1),
                   e = runif(n,0,1),
                   y = 1+x+e)
  reg <- lm(y ~ x, data = df)
  rs <- coeftest(reg, vcov=vcovHC(reg, type="const")) %>% 
    tidy %>%
    filter(term=="x") %>%  # keep only stats about beta_1
    select(estimate) %>%   # keep estimate
    mutate(n = n, asymp_dist = sqrt(n)*(estimate - 1)) %>%
    select(n,everything())
  return(rs)
}

Simu_beta(10) # test output
S <- 2000
size_gr <- expand.grid(1:S, n=c(10,100,500))
simu_gr <- mapply(Simu_beta, n=size_gr$n, SIMPLIFY = FALSE) %>%
  bind_rows() %>%
  as_data_frame()
```
## Convergence in Probability
```{r consistency}
ggplot(aes(x=estimate), data=simu_gr) +
  geom_density()+
  facet_grid(.~n)
```

## Convergence in Distribution
```{r distribution}
ggplot(aes(x=asymp_dist), data=simu_gr) +
  geom_density() +
  facet_grid(.~n)
```

## Explanation

- The first graph shows the density plots of $\hat{\beta}$ for three cases (with different sample sizes); The second graph shows the correponding distributions of $\sqrt{n}(\hat{\beta}-\beta)$.

- From the first graph we can see that with larger sample size, the distribtuion of our estimate will shrink to its mean. That is because OLS estimator is consistent and its variance will shrink to zero as n goes to infinity.

- However, if we scale up the distance between estimator and its mean, we can get a nondegenerate distribution of $\hat{\beta}$, and with larger sample size, by Central Limit Theorem, the distribution will converge to a normal distribution.

## Augustin-Louis Cauchy
```{r Cauchy, results = "hide"}
rm(list = ls())
N <- 10000
dgp <- function(dist_err){
  set.seed(10101)
  if(dist_err == "normal"){
    err = rnorm(N,0,1)
  }
  else if(dist_err == "cauchy"){
    err = rcauchy(N,0,1)
  }
  df <- data_frame(x = rnorm(N,0,1),
                   y = 0.8 + 0.3*x + err,
                   dist_err = dist_err)
  return(df)
}
head(dgp("normal"),10) # test the output

sample <- mapply(dgp, dist_err=c("normal","cauchy"), SIMPLIFY = FALSE) %>%
  bind_rows() %>%
  as_data_frame()

Mycoef <- function(n,d){
  dat <- filter(sample, dist_err == d)%>%
    head(n = n)
  reg <- lm(y ~ x, data = dat)
  beta <- coeftest(reg, vcov=vcovHC(reg, type="const")) %>% 
    tidy %>%
    filter(term=="x") %>%
    select(estimate) %>%
    mutate(n = n, dist_e = d)
  return(beta)
}

Mycoef(100, "normal")  # test the output

size_gr <- expand.grid(n = seq(10,N, by = 10), d=c("normal","cauchy"))
simu_gr <- mapply(Mycoef, n=size_gr$n, d = size_gr$d, SIMPLIFY = FALSE) %>%
  bind_rows() %>%
  as_data_frame()
```

```{r statistics}
stats_beta <- simu_gr %>%
  group_by(dist_e) %>% 
  summarise(mean=mean(estimate),
            sd = sd(estimate))
kable(stats_beta, digits = 3, caption = "Mean and sd of OLS coefficients")
```

- In the Normal case, the mean of $\hat{\beta}$ is close to its true value, and the standard deviation is close to zero.
- In the Cauchy case, the mean of $\hat{\beta}$ is by no mean close to the true value, and the standard deviation is still pretty large.

## The OLS slope estimator

```{r slope}
ggplot(aes(x=n, y=estimate, colour = dist_e), data=simu_gr) +
  geom_point() +
  facet_grid(. ~ dist_e) +
  geom_hline(aes(yintercept= mean(estimate)),
             data <- simu_gr %>%
               filter(dist_e == "normal")) +
  geom_hline(aes(yintercept= mean(estimate)),
             data <- simu_gr %>%
               filter(dist_e == "cauchy"))
```

## Comments

- To get consistency, we need the following assumptions:
1. $(y_i,x_i)$ sampled independently from F;
2. $E(x_ix_i')$ is positive definite;
3. $E(y_i^2) < \infty, E(||x_i||^2) < \infty$

- In the Normal case, all these assumptions are satisfied, then by WLLN and CMT, $\hat{\beta}$ is consistent, that is, as sample size gets larger, it will converge to its mean.

- However, in the Cauchy case, since the Cauchy distribution does not have finite moments of order greater than or equal to one, this violates finite second moment for y (because of e). Thus, we can say little about the mean and sd of $\hat{\beta}$.

## Appendix
In the proof of consisitency, we want
$$ \hat{\beta} \overset{p}{\rightarrow} E(x_ix_i')^{-1}E(x_iy_i) $$
By Holder's Inequality,
$$E(x_iy_i) \leq E(||x_iy_i||) \leq E(||x_i||^2)^{1/2}E(y_i^2)^{1/2}$$
In the Cauchy case, $E(x_iy_i)$ may not be bounded since y has no finite second moment because of e.

