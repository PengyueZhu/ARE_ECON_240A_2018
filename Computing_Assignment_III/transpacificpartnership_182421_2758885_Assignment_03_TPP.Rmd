---
title: 'Assignment 3: Distribution of OLS Estimator'
output:
  slidy_presentation: default
  ioslides_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## X-Y Plot with Leverage

- Here we rerun the simulation from Assignment II, for n=100, and for the $X \sim \chi^2$ distribution (s=1). 
- We estimate a linear regression for this simulation and show an x-y scatter plot below, where size depends on the leverage value. 
- Note that leverage of x observations increase as x increases, which is consistent with our understanding that the linear estimate of y has greater errors as x increases (since the conditional expectation of y is non-linear). 


## X-Y Plot with Leverage
```{r, echo = FALSE, warning=FALSE}
suppressPackageStartupMessages(library(tidyverse))
library(tidyverse)
library(ggplot2)

n=100
set.seed(12345)
x<-rchisq(n,1)
e<-rnorm(n)
y<-(x*x)+e
reg<-lm(y~x)

X<-data_frame(x,e,y)
leverage<-hatvalues(reg)
data<-mutate(X,leverage)

xy <- ggplot(aes(x=x, y=y, size=leverage), data=data)+
geom_point()+
geom_smooth(method="lm", se=FALSE, show.legend=FALSE)+
annotate("text", x = 5.5, y = 20, label = "hat(y) == 4.49*X-1.36", parse = TRUE)
plot(xy)
```

## Distribution of OLS estimator with HC covariance

- Here we seek to understand whether bias of estimator $\hat{\beta}_1$ stems from standard error estimation or distribution approximation.

- The model in question is $y_i = \beta_0 + \beta_1 D_i + \epsilon_i$, with $D_i = {0,1}$

- First we show tabular results for the heteroskedastic $\beta_1$ estimate and then the homoskedastic results  

## Distribution of OLS estimator - High heteroskedasticity

```{r, echo=FALSE, message=FALSE, warning=FALSE}
suppressMessages(library(tidyverse))
suppressMessages(library(zoo))
suppressMessages(library(base))
library(ggplot2)
library(tidyverse)
library(lmtest)
library(sandwich)
library(knitr)
rm(list=ls())
n=30
k=3


##create empty matrix 
X<-matrix(NA, nrow = n, ncol = 3)

##generate D
D0<-matrix(rep(0,n-k),ncol = 1)
D1<-matrix(rep(1,k),ncol = 1)
D<-rbind(D0,D1)


X[,1]<-D ## the first column in X is D


reg<- function(n,h){  
  
  for (i in 1:n){       
    if (X[i,1]==1){
      X[i,2]=rnorm(1,0,1)
    }else{
      X[i,2]=rnorm(1,0,h)
    }
    X[i,3]=X[i,2]
  }
  reg1<-lm(X[,3]~X[,1]) ## regress y on D
  
  beta<-matrix(summary(reg1)$coef[2,1])  ## estimated slope
  homo<-matrix(sqrt(diag(vcovHC(reg1, type = "const"))))[2] ## standard error assuming homoskedascity
  hc1<-matrix(sqrt(diag(vcovHC(reg1, type = "HC1"))))[2] ## standard error using HC0
  hc_ora<-matrix(sandwich(reg1, meat. = meatHC(reg1, omega=X[,1])))[2] ## stadard error using unfeasible/oracle HC
  p_value_con<-matrix(summary(reg1)$coef[2,4]) ## p_value using con
  p_value_hc1<-coeftest(reg1, vcov = vcovHC(reg1, type="HC1"))[2,4] ## p_value using HC1
  p_value_ora<-coeftest(reg1, vcov = sandwich(reg1, meat. = meatHC(reg1, omega=X[,1])) )[2,4]
  t_value_con<-matrix(beta/summary(reg1)$coef[2,2])
  t_value_hc1<-matrix(beta/coeftest(reg1, vcov = vcovHC(reg1, type="HC1"))[2,2])
  t_value_ora<-matrix(beta/coeftest(reg1, vcov = sandwich(reg1, meat. = meatHC(reg1, omega=X[,1])) )[2,2])
  return(matrix(c(beta,homo,hc1,hc_ora,p_value_con,p_value_hc1,p_value_ora,t_value_con,t_value_hc1,t_value_ora)))
}



##create empty matrix to store simulation results

Sim_hete<-matrix(NA,nrow=10,ncol=10000)
Sim_homo<-matrix(NA,nrow=10,ncol=10000)  



##run Monte_carlo
for (i in 1:10000){
  Sim_hete[,i]<-matrix(reg(30,0.5))
  Sim_homo[,i]<-matrix(reg(30,1))
}

##create empty matrix that will store the summary of the simulations

Result_hete<-matrix(NA,nrow=10,ncol=2)
Result_homo<-matrix(NA,nrow=10,ncol=2)


##store the results generated by simulations

for (i in c(1:4)){
  Result_hete[i,1]<-mean(Sim_hete[i,])
  Result_hete[i,2]<-sd(Sim_hete[i,])
  Result_homo[i,1]<-mean(Sim_homo[i,])
  Result_homo[i,2]<-sd(Sim_homo[i,])
}

for (i in c(5:7)){
  Result_hete[i,1]<-length(which(Sim_hete[i,]<=0.05))/10000
  Result_homo[i,1]<-length(which(Sim_homo[i,]<=0.05))/10000
}

for (i in c(8:10)){
  Result_hete[i,1]<-mean(Sim_hete[i,])
  Result_hete[i,2]<-sd(Sim_hete[i,])
  Result_homo[i,1]<-mean(Sim_homo[i,])
  Result_homo[i,2]<-sd(Sim_homo[i,])
}
colnames(Result_hete)<-c("mean","standard deviation")
colnames(Result_homo)<-c("mean","standard deviation")
rownames(Result_hete)<-c("beta1","Homo","HC1","unfeasible/oracle","p_value_homo","p_value_HC1","p_value_Ora","t_value_homo","t_value_HC1","t_value_Ora")
rownames(Result_homo)<-c("beta1","Homo","HC1","unfeasible/oracle","p_value_homo","p_value_HC1","p_value_Ora","t_value_homo","t_value_HC1","t_value_Ora")

## final result

kable(round(Result_hete[1:7,], 3))
```

## Distribution of OLS estimator - No heteroskedasticity

```{r, warning=FALSE}
suppressPackageStartupMessages(library(tidyverse))
kable(round(Result_homo[1:7,],3))
```


## Distribution of t-ratios
```{r, echo=FALSE, warning=FALSE,}
suppressPackageStartupMessages(library(tidyverse))
#generate the graph of empirical and theoretical t distributions

tmp = NULL

indicator = rep("Homo_conv", 10000)

tmp = cbind(Sim_homo[8 ,], indicator)

tmp = rbind(tmp, cbind(Sim_homo[9,  ], rep("Homo_HC1", 10000)))

tmp = rbind(tmp, cbind(Sim_hete[8, ], rep("Hete_conv", 10000)))

tmp = rbind(tmp, cbind(Sim_hete[ 9,  ], rep("Hete_HC1", 10000)))

Theo_t<-rt(10000, 28)

tmp = rbind(tmp, cbind(Theo_t,rep("theoretic", 10000)))

tmp = data.frame(tmp)

colnames(tmp) = c("t_ratio", "indicator")

tmp$t_ratio = as.numeric(as.character(tmp$t_ratio))

ggplot(tmp, aes(t_ratio, colour = factor(indicator))) + geom_density() + xlim(-7, 7)
```

## Comments
Asymptotic standard errors are given by the square root of the diagonal elements of omega. In finite samples, both variance estimators are biased. If the residuals are homoscedastic, the robust estimator (in our case, HC1) is more biased than the conventional one. 

Consider $E[{\hat{\Omega}} _c]=(X'X)^{-1}\sigma^2=(X'X)^{-1}(\sum \frac{E({\hat {e}}_i^2)}{N})$. Under homoscedasticity, $\sum \frac{E({\hat {e}}_i^2)}{N}=\sigma^2\sum \frac{1-h_{ii}}{N}=\sigma^2(\frac{N-K}{N})$. The bias in the conventional estimator is caused by the leverage of observations far from the mean, and can be fixed by a degree-of-freedom correction. On the other hand, the expected value of the robust covariance matrix estimator is $E[{\hat{\Omega}}_r]=N(X'X)^{-1}(\sum\frac{X_iX_i'E({\hat {e}}_i^2)}{N})(X'X)^{-1}$. Therefore, the bias in  ${\hat {e}}_i^2$ will pull the robust standard errors down, and the estimator will be more biased. 

From our simulation under homoscedasticity, we indeed find the conventional standard error to be close to the standard deviation of the beta1 coefficient, while the HC1 standard error is much more biased downward. This is due to the small sample bias and their higher sampling variance. In our sample, if y is normal with equal (but unknown) variance in both the D=1 and D=0 populations, i.e. with homoscedastic residuals, then the conventional t-ratio for the beta1 coefficient has a t-distribution. This is verified by our graph, in which the Homo_conv distribution is almost identical to the theoretical student t-distribution. The rejection rate in this scenario equals 5%, while the rejection under Homo_HC1 is much larger. 

## Comments
In the case of heteroscedasticity, however, the result is quite the opposite. The bias in the HC1 estimator is smaller than the conventional estimator. In Mostly Harmless the authors argue that “Robust standard errors improve on conventional standard errors because the resulting inferences are asymptotically valid when the regression residuals are heteroskedastic.” Our result confirms this argument that with lots of heteroskedasticity, the conventional standard errors are badly biased, while the robust standard errors perform better but still much smaller when compared to the standard deviation of the beta1 coefficient. 

When the variances in the D=1 and D=0 populations are different (heteroskedasticity), the exact small-sample distribution is unknown. The robust variance estimator HC1 performs better in giving asymptotic approximations to the unknown distribution. Since the conventional estimator pools subsamples together, it is efficient when the two variances are the same, but less efficient when the two variances are indeed different. Thus, we can see here that the Hete_conv is the worst approximation of the t-test distribution, giving out an unrealistically high rejection rate. 

To sum, the bias issue under homoscedasticity mostly comes from the use of a biased estimator of the variance. Under heteroskedasticity, the bias is mainly caused by the use of a bad approximation of the t-test distribution, but the use of a biased estimator of the variance also plays a role. 

