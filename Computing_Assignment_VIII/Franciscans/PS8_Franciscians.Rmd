---
title: "PS 8 Franciscians"
author: "Team Francis"
date: "3/12/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(MASS)
library(broom)
library(modelr)
library(tidyverse)
library(kableExtra)
```

```{r echo=FALSE}
remove(list = ls())
set.seed(10101)
Sigma <-matrix(c(1, 0.7, 0.7, 1), 2)

sim_reg <- function(n=100, B=200, s=s){
  X <- mvrnorm(n=n, c(0, 0), Sigma = Sigma)
  y <- 1.2 + 0.3*X[,1]+1.1*X[,2]+rnorm(n)
  df <- data_frame(y=y, X1=X[,1], X2=X[,2])

  ## reg
  reg <- lm(y~X1+X2, data=df)
  reg_su <- summary(reg)
  reg_co <- tidy(reg_su)

  co_X1 <- reg_co$estimate[2]

  co_X2 <- reg_co$estimate[3]

  ratio <- co_X1/co_X2

  ## Delta

  # R <- COMPUTE IT for the three parameters
  R <- c(0, 1/co_X2, -co_X1/(co_X2)^2)
  v <- as.numeric(t(R) %*% vcov(reg_su) %*% R)

  # W-test

  # W_test <- COMPUTE IT
  W_test <- (ratio-3/11)*v^-1*(ratio-3/11)
  ## boot it:

  x_boot <- modelr::bootstrap(df, n=B) %>%
    mutate(reg=map(strap, ~lm(y~X1+X2, data=.)),
           reg_su = map(reg, summary),
           reg_co = map(reg_su, tidy),
           ratio_b = map_dbl(reg_co, ~.$estimate[2]/.$estimate[3]),
           R = map(reg_co, ~c(0, 1/.$estimate[3], -.$estimate[2]/(.$estimate[3])^2) ), 
           vcov_b = map2_dbl(reg_su, R, ~ t(.y) %*% vcov(.x) %*% .y),
           W_test_b = map2_dbl(ratio_b, vcov_b, ~(.x-3/11)*solve(.y)*(.x-3/11)),
           W_test_b2 = map2_dbl(ratio_b, vcov_b, ~(.x-ratio)*solve(.y)*(.x-ratio)))
  x_boot2 <- x_boot %>%
  summarise(boot_sd=sd(ratio_b),
            boot_sd_v = mean(vcov_b),
            boot_mean=mean(ratio_b),
            wtest_cval=quantile(W_test_b, .95),
            wtest2_cval=quantile(W_test_b2, .95))
  x_boot2
  ## results: original Wald and va, and boot aggregates
  data_frame(co_X1=co_X1, co_X2=co_X2, ratio=ratio,
             W_test=W_test,
             v=v) %>%
    bind_cols(x_boot2)
  }

```

```{r echo=FALSE}
set.seed(10101)

program_gr <- expand.grid(s= 1:200)
res_gr<-mapply(sim_reg, s=program_gr$s, SIMPLIFY = FALSE) %>%
  bind_rows() %>%
  as_data_frame() 
```

## Q1
```{r echo=FALSE}
## Calculating asymptotic
R_true <- c(0, 1/1.1, -.3/(1.1)^2)
Sigma_true <-matrix(c(1, 0, 0, 0, 1, 0.7, 0, 0.7, 1), 3)
Asympt_variance <- t(R_true)%*%solve(Sigma_true)%*%R_true/100
```
When run with S=200, the bootstrap variance is `r mean(res_gr$v)`, which is slightly larger than the exact variance  of `r var(res_gr$ratio)`. Both are larger than the asymptotic variance of `r Asympt_variance`. 

## Q2
```{r echo=FALSE} 
wt <- quantile(res_gr$W_test, 0.95)
wa <- qchisq(.95, df=3)
```
The 95% quantile of the Wald test (`r wt`) is much lower than that of the asymptotic distribution (`r wa`).

## Q3
```{r echo=FALSE}
Rej_rate <- as.numeric(res_gr$W_test>qchisq(.95, df=3))
```
Average rejection rate of the Wald tests are given by `r mean(Rej_rate)`.

## Q4
```{r echo=FALSE}
W_BS_Error1 <- (res_gr$ratio - 3/11)*res_gr$boot_sd_v^-1*(res_gr$ratio - 3/11)
Rej_rate_BS1 <- as.numeric(W_BS_Error1>qchisq(.95, df=3))
```
We calculate a Wald stat using the ratio estimate from each sample (the ratio variable) and use the bootstrap variance estimate (boot_sd_v). Using this test the rejection rate is `r mean(Rej_rate_BS1)`.

## Q5
```{r echo=FALSE, warning=FALSE}
W_BS_Error2 <- (res_gr$ratio - 3/11)*Asympt_variance^-1*(res_gr$ratio - 3/11)
Rej_rate_BS2 <- as.numeric(W_BS_Error2>res_gr$wtest_cval)
Rej_rate_BS3 <- as.numeric(W_BS_Error2>res_gr$wtest2_cval)
```
We run the second bootstrap test in which we use the Wald statistic using asymptotic variance estimator (please see Q1). Using this test the rejection rate is `r mean(Rej_rate_BS2)`. We then use critical values calculated in the bootstrap procedure, both critical values calculated (one centered around beta1/beta2, the other betahat1/betahat2). Using this test the rejection rate is `r mean(Rej_rate_BS3)`.

## Q6
Because the critical values found using the Wald statistic centered around the estimated value is smaller than the one centered around the true value (for each sample on which we run bootstrap), we expect that the former would have a higher rejection rate than the latter.

## Q7
The result would likely change, because the Delta method is sensitive to the way in which we define the restrictions. 


## Cross Validation Section
```{r echo=FALSE}
select <- dplyr::select

## generate data
n <- 100
S <- matrix(c(1, 0.5, 0.5, 1), 2)
set.seed(2018)

X <- mvrnorm(n = n, mu=c(0,0), Sigma=S)
y <- 1.2 + 0.3 * X[,1]+rnorm(n)
data_df <- data_frame(y=y, x1=X[,1], x2=X[,2])

## cross validation
crossval_df <- crossv_kfold(data_df, k = n)

crossval <- crossval_df %>%
  mutate(reg1 = map(train, ~lm(y~x1, data=.)),
         reg2 = map(train, ~lm(y~x1+x2, data=.))) %>%
  gather(model, reg, reg1, reg2) %>%
  mutate(resids_out = map2_dbl(reg, test, ~ predict(.x, newdata=.y)-as_data_frame(.y)$y),
         mse_in = map_dbl(reg, ~mean(residuals(.)^2)),
         mse_out = map_dbl(resids_out, ~mean(.^2))) %>%
gather(mse, value, mse_in, mse_out)
```


#1
The dimension of crossval_df is 100x3. Each cell contains a 99x3 or 1x3 list of data: each row is a resample of all three variables of data_df where 99 of the observations are in the column train and the last observation is in the column 1. The last column is an ID variable for the resample. 
Using the data in the train column, both regressions are run on each of the resamples. The data are reshaped so the regressions are contained in the "reg" column and the "model" column is a label for reg1 or reg2. Then the in-sample MSE is calculated of each of these regressions (mse_in). The "resids_out = map2_dbl(...)" line generates the residual from the prediction of the out of sample observation using the regression with rest of the data. The .x and .y refer to the first two arguments of map2_dbl: reg (the regresson with the training data) and test (the out of sample observation). Then the out of sample MSE is calculated from this residual. Finally the data are reshaped so that both MSEs are in one column (value) and the mse column labels whether the value is mse_in vs. mse_out. 
```{r echo=FALSE}
dim <- dim(crossval_df)
```

#2 
The in-sample MSE is higher for for the regression with just x1, which makes sense because it has one less explanatory variable. However, the out-sample MSE is similar for both regressions (only slightly higher for the regression with just x1) -- this is because in fact the d.g.p. only contains x1 and not x2, so adding x2 is overfitting the data. For both models, the out-sample MSE is higher than the in-sample MSE because the data are overfitted. 
```{r echo=FALSE}
crossval %>% 
  group_by(model, mse) %>% 
  summarise(mean=mean(value)) %>% 
  kable(.,caption = "In- and Out-sample MSE by regression", digits = 3)
```

#3 
The Hansen formula out-sample MSE estimates are almost identical to our simulation estimates. 
```{r echo=FALSE}
reg1 <- lm(y~x1, data=data_df)
reg2 <- lm(y~x1+x2, data=data_df)

mse_out1 <- mean((1-hatvalues(reg1))^-2 * residuals(reg1)^2)
mse_out2 <- mean((1-hatvalues(reg2))^-2 * residuals(reg2)^2)
```
The formula estimate is `r mse_out1` for regression 1 and `r mse_out2` for regression 2. 
